{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MasaCtrl: Tuning-free Mutual Self-Attention Control for Consistent Image Synthesis and Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "from masactrl.diffuser_utils import MasaCtrlPipeline\n",
    "from masactrl.masactrl_utils import AttentionBase\n",
    "from masactrl.masactrl_utils import regiter_attention_editor_diffusers\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "torch.cuda.set_device(0)  # set the GPU device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction\n",
    "\n",
    "load Dreambooth tuned StableDiffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'rescale_betas_zero_snr': False, 'timestep_spacing': 'leading'} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "The config attributes {'addition_embed_type': None, 'addition_embed_type_num_heads': 64, 'addition_time_embed_dim': None, 'attention_type': 'default', 'encoder_hid_dim_type': None, 'num_attention_heads': None, 'time_embedding_dim': None, 'transformer_layers_per_block': 1} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "Some weights of the model checkpoint at /root/DreamMatcher/concept_models/dreambooth/dreambooth-concept-original-prior-230821/black_cat/checkpoints/diffusers/vae were not used when initializing AutoencoderKL: ['encoder.mid_block.attentions.0.to_out.0.bias', 'decoder.mid_block.attentions.0.to_q.weight', 'decoder.mid_block.attentions.0.to_k.weight', 'encoder.mid_block.attentions.0.to_q.weight', 'encoder.mid_block.attentions.0.to_out.0.weight', 'decoder.mid_block.attentions.0.to_v.weight', 'decoder.mid_block.attentions.0.to_k.bias', 'encoder.mid_block.attentions.0.to_k.weight', 'encoder.mid_block.attentions.0.to_v.bias', 'encoder.mid_block.attentions.0.to_k.bias', 'encoder.mid_block.attentions.0.to_q.bias', 'decoder.mid_block.attentions.0.to_q.bias', 'decoder.mid_block.attentions.0.to_v.bias', 'encoder.mid_block.attentions.0.to_v.weight', 'decoder.mid_block.attentions.0.to_out.0.bias', 'decoder.mid_block.attentions.0.to_out.0.weight']\n",
      "- This IS expected if you are initializing AutoencoderKL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AutoencoderKL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AutoencoderKL were not initialized from the model checkpoint at /root/DreamMatcher/concept_models/dreambooth/dreambooth-concept-original-prior-230821/black_cat/checkpoints/diffusers/vae and are newly initialized: ['decoder.mid_block.attentions.0.proj_attn.bias', 'decoder.mid_block.attentions.0.query.weight', 'decoder.mid_block.attentions.0.key.weight', 'encoder.mid_block.attentions.0.proj_attn.weight', 'decoder.mid_block.attentions.0.value.weight', 'encoder.mid_block.attentions.0.query.bias', 'encoder.mid_block.attentions.0.key.bias', 'decoder.mid_block.attentions.0.query.bias', 'encoder.mid_block.attentions.0.proj_attn.bias', 'encoder.mid_block.attentions.0.query.weight', 'encoder.mid_block.attentions.0.value.bias', 'decoder.mid_block.attentions.0.key.bias', 'decoder.mid_block.attentions.0.value.bias', 'decoder.mid_block.attentions.0.proj_attn.weight', 'encoder.mid_block.attentions.0.key.weight', 'encoder.mid_block.attentions.0.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You have disabled the safety checker for <class 'masactrl.diffuser_utils.MasaCtrlPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "# DB tuned\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_path = \"/root/DreamMatcher/concept_models/dreambooth/dreambooth-concept-original-prior-230821/black_cat/checkpoints/diffusers\" \n",
    "\n",
    "scheduler = DDIMScheduler.from_pretrained(\n",
    "        model_path, subfolder=\"scheduler\", low_cpu_mem_usage=False\n",
    "    )\n",
    "\n",
    "model = MasaCtrlPipeline.from_pretrained(\n",
    "    model_path,\n",
    "    low_cpu_mem_usage=False,\n",
    "    scheduler=scheduler,\n",
    "    safety_checker=None,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistent synthesis with MasaCtrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting step : 2, Starting layer : 10\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasaCtrl at denoising steps:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:12<00:00,  3.92it/s]\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntheiszed images are saved in ./workdir/masactrl_db_2_10/sample_0\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasaCtrl at denoising steps:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntheiszed images are saved in ./workdir/masactrl_db_2_10/sample_1\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasaCtrl at denoising steps:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntheiszed images are saved in ./workdir/masactrl_db_2_10/sample_2\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasaCtrl at denoising steps:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntheiszed images are saved in ./workdir/masactrl_db_2_10/sample_3\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasaCtrl at denoising steps:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntheiszed images are saved in ./workdir/masactrl_db_2_10/sample_4\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasaCtrl at denoising steps:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntheiszed images are saved in ./workdir/masactrl_db_2_10/sample_5\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:10<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasaCtrl at denoising steps:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n",
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:12<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntheiszed images are saved in ./workdir/masactrl_db_2_10/sample_6\n"
     ]
    }
   ],
   "source": [
    "from masactrl.masactrl import MutualSelfAttentionControl\n",
    "\n",
    "prompts_list = [ \n",
    "    [\n",
    "        \"sks black_cat, casual, outdoors, laying\",  # source prompt\n",
    "        \"sks black_cat, casual, outdoors, standing\"  # target prompt\n",
    "    ],\n",
    "    [\n",
    "        \"sks black_cat, casual, outdoors, laying\",  # source prompt\n",
    "        \"cat, casual, outdoors, standing\"  # target prompt\n",
    "    ],\n",
    "    [\n",
    "        \"sks black_cat, casual, outdoors, laying\",  # source prompt\n",
    "        \"cat, standing\"  # target prompt\n",
    "    ],\n",
    "    [\n",
    "        \"sks black_cat, casual, outdoors, laying\",  # source prompt\n",
    "        \"fat furry white cat, standing\"  # target prompt\n",
    "    ],\n",
    "    [\n",
    "        \"sks black_cat, casual, outdoors, laying\",  # source prompt\n",
    "        \"cat jumping in the sky\"  # target prompt\n",
    "    ],\n",
    "    [\n",
    "        \"sks black_cat, casual, outdoors, laying\",  # source prompt\n",
    "        \"cat running in the garden\"  # target prompt\n",
    "    ],\n",
    "    [\n",
    "        \"sks black_cat, casual, outdoors, laying\",  # source prompt\n",
    "        \"cat in the box\"  # target prompt\n",
    "    ]\n",
    "]\n",
    "\n",
    "# inference the synthesized image with MasaCtrl\n",
    "# STEP = 6\n",
    "# LAYPER = 10\n",
    "start_step_layer = [\n",
    "    (2, 10),\n",
    "    (4, 10),\n",
    "    (6, 10)\n",
    "]\n",
    "\n",
    "for STEP, LAYPER in start_step_layer:\n",
    "    out_dir = f\"./workdir/masactrl_db_{STEP}_{LAYPER}/\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f\"Starting step : {STEP}, Starting layer : {LAYPER}\")\n",
    "    \n",
    "    for prompts in prompts_list:\n",
    "        seed = 42\n",
    "        seed_everything(seed)\n",
    "\n",
    "        sample_count = len(os.listdir(out_dir))\n",
    "        sample_dir = os.path.join(out_dir, f\"sample_{sample_count}\")\n",
    "        os.makedirs(sample_dir, exist_ok=True)\n",
    "        \n",
    "        # initialize the noise map\n",
    "        start_code = torch.randn([1, 4, 64, 64], device=device)\n",
    "        start_code = start_code.expand(len(prompts), -1, -1, -1)\n",
    "\n",
    "        # inference the synthesized image without MasaCtrl\n",
    "        editor = AttentionBase()\n",
    "        regiter_attention_editor_diffusers(model, editor)\n",
    "        image_ori = model(prompts, latents=start_code, guidance_scale=7.5)\n",
    "\n",
    "        # hijack the attention module\n",
    "        editor = MutualSelfAttentionControl(STEP, LAYPER)\n",
    "        regiter_attention_editor_diffusers(model, editor)\n",
    "\n",
    "        # inference the synthesized image\n",
    "        image_masactrl = model(prompts, latents=start_code, guidance_scale=7.5)[-1:]\n",
    "\n",
    "        # save the synthesized image\n",
    "        out_image = torch.cat([image_ori, image_masactrl], dim=0)\n",
    "        save_image(out_image, os.path.join(sample_dir, f\"all_step{STEP}_layer{LAYPER}.png\"))\n",
    "        save_image(out_image[0], os.path.join(sample_dir, f\"source_step{STEP}_layer{LAYPER}.png\"))\n",
    "        save_image(out_image[1], os.path.join(sample_dir, f\"without_step{STEP}_layer{LAYPER}.png\"))\n",
    "        save_image(out_image[2], os.path.join(sample_dir, f\"masactrl_step{STEP}_layer{LAYPER}.png\"))\n",
    "\n",
    "        print(\"Syntheiszed images are saved in\", sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "587aa04bacead72c1ffd459abbe4c8140b72ba2b534b24165b36a2ede3d95042"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
